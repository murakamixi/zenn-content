---
title: "Sudachiã‚’ã¤ã‹ã£ãŸBERTã®äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã‚’ã—ã¦ã¿ã‚ˆã†ã¨ãŠã‚‚ã£ãŸã‚‰ï¼ˆã„ã¾ã‚„ã£ã¦ã‚‹é€”ä¸­ï¼‰"
emoji: "ğŸ™„"
type: "tech"
topics: ["bert", "sudachi", "transformer", "äº‹å‰å­¦ç¿’", "sudachitra"]
published: true
---

ã“ã‚Œã¯å€‹äººçš„ãªä½œæ¥­è¨˜éŒ²çš„ãªã‚‚ã®ã¨ã€ã‚‚ã—ä»Šå¾ŒåŒã˜ã‚ˆã†ãªå¿—ã‚’æŒã£ãŸäººã®å‚è€ƒã¨ãªã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã«å¯èƒ½ãªé™ã‚Šä½œæ¥­ã®è©³ç´°ã‚’æ®‹ã—ã¦ã„ã¾ã™ã€‚

# ãã‚‚ãã‚‚Sudachiã¨ã¯
ã€€ãã‚‚ãã‚‚Sudachiã£ã¦ãªã«ã¦ããªã²ã¨ã¯ã“ã‚“ãªãƒãƒ‹ã‚¢ãƒƒã‚¯ãªã‚¿ã‚¤ãƒˆãƒ«ã®è¨˜äº‹ã®ã¨ã“ã‚ã«æ¥ãªã„ã§ã—ã‚‡ã†ãŒã‚‚ã—ã‚‚æ°—ã«ãªã‚‹æ–¹ã¯åƒ•ã®å‹äººãŒã€[å½¢æ…‹ç´ è§£æå™¨Sudachiã‚’pythonãƒ•ã‚¡ã‚¤ãƒ«ã§ä½¿ã£ã¦ã¿ãŸ](https://zenn.dev/2timesbottle/articles/4c0f3a4ce26797)çš„ãªè¨˜äº‹ã‚’æ›¸ã„ã¦ã„ã‚‹ã®ã§çš„ã«å‚ç…§ã—ã¦ã¿ã‚‹ã®ãŒã„ã„ã¨ãŠæ€ã„ã¾ã™ã€‚
 
# ãªã‚“ã§äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãªã‚“ã‹ä½œã‚ã†ã¨æ€ã£ãŸã®ã‹ï¼Ÿ
 ã€€åœ°æ–¹å›½ç«‹å¤§ã®ç†å­¦éƒ¨ã«åœ¨ç±ã—ã¦ã„ã¦ã€ã¡ã‚‡ã†ã©å’æ¥­ç ”ç©¶çµ‚ã‚ã‚Šãªã©ã§ã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ãŒã»ã¼ç‹¬å çŠ¶æ…‹ã§ã¤ã‹ãˆã‚‹ã“ã¨
  è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚‚ä¸€å¿œå›½ç«‹å¤§ã¨ã„ã†ã ã‘ã‚ã£ã¦BERTã®äº‹å‰å­¦ç¿’ã«è€ãˆã‚‰ã‚Œã‚‹ã ã‘ã‚ã‚Šãã†
  ç ”ç©¶å®¤é…å±å‰ã§ã‚ã‚Šã¨å¥½ããªã“ã¨ãŒã§ãã¦ã€æ™‚é–“ãŒã‚ã‚‹æœŸé–“ã¨ã„ã†ã“ã¨
  
  ã¨ã„ã†æ„Ÿã˜ã«ã¹ã¤ã«çµ¶å¯¾ã‚„ã‚Šé‚ã’ã‚ˆã†ã¨ãŠã‚‚ã£ã¦ã‚‹ã‚ã‘ã§ã‚‚ãªã„ã®ã§ã™ãŒã€ãªã«ã‚ˆã‚Šã€Œ**ãªã„ã‹ã‚‰ãªã‚“ã¨ãªãã¤ãã£ã¦ã¿ã‚ˆã†ã‹ãª**ã€ã¦ããªèˆˆå‘³æœ¬ä½ã§ã¯ã˜ã‚ã¾ã—ãŸã€‚
  
# æ‰‹é †
  
  ã€€æ‰‹é †ã¯ã™ã¹ã¦[Training Sudachi BERT Models](https://github.com/WorksApplications/SudachiTra/tree/update/pretraining_document/pretraining/bert)ã«æ›¸ã„ã¦ã‚ã‚Šã¾ã™ã€‚
   ã„ã‚ã„ã‚ã¨å¤ã„ã‚‚ã®ã‚‚ç‰çŸ³æ··åˆã ã£ãŸã‚ˆã†ã§ã™ãŒã€Slackä¸Šã§èãã¨ä¸å¯§ã«å¯¾å¿œã—ã¦ã„ãŸã ã‘ã‚‹é–‹ç™ºè€…ã®æ–¹ãŒã„ã¦å¤ã„ã‚‚ã®ã‚„ã‚¨ãƒ©ãƒ¼ã¯ä¸€æƒã•ã‚ŒãŸã‚ˆã†ã«æ€ãˆã¾ã™ã€‚
   ã¾ã ã‚„ã£ã¦ã‚‹é€”ä¸­ãªã®ã§ã‚ã‹ã‚Šã¾ã›ã‚“ï¼
   
   ä»¥ä¸‹ã¯ä¸Šè¨˜ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ—ãƒ©ã‚¹å€‹äººçš„ãªç’°å¢ƒã®ã‚‚ã®ãªã©å…¥ã‚Œã¦ã¾ã™ã€‚

[ç’°å¢ƒæ§‹ç¯‰](ç’°å¢ƒæ§‹ç¯‰)
[å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)
   
# ç’°å¢ƒæ§‹ç¯‰

ç’°å¢ƒæ§‹ç¯‰ã¯ã€
1.SudachiTraã‚’Githubã‹ã‚‰Clone
2.Dockerã‚³ãƒ³ãƒ†ãƒŠã®æº–å‚™
ã§è¡Œã„ã¾ã™ã€‚

## SudachiTraã‚’Githubã‹ã‚‰Clone

ã€€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãªã„ã¨å§‹ã¾ã‚‰ãªã„ã®ã§ã€[Training Sudachi BERT Modelsã€€set-up](https://github.com/WorksApplications/SudachiTra/tree/update/pretraining_document/pretraining/bert#set-up)ã®ã¯ã˜ã‚ã«ã‚ã‚‹

```bash : bash
git clone --recursive https://github.com/WorksApplications/SudachiTra/
pip install -U sudachitra
```

ã®ã‚³ãƒãƒ³ãƒ‰ã‚’é©å½“ãªã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§å®Ÿè¡Œã—ã¾ã™ã€‚

::: message
ãã®å¾Œã€ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç’°å¢ƒæ§‹ç¯‰ç”¨ã®Dockerã‚³ãƒ³ãƒ†ãƒŠã®æº–å‚™ã‚’ã—ã¾ã™ã€‚
:::

## Dockerã®æº–å‚™
 ã€€ç’°å¢ƒã¯ã€[NGCãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®Tensorflowã‚³ãƒ³ãƒ†ãƒŠ](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow)ã‚’åˆ©ç”¨ã—ã¾ã—ãŸã€‚
 ã€€Dockerfileã¨docker-compose.ymlã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ„Ÿã˜ã§ã™ã€‚
  
  ```Dockerfile:SudachiTra/Dockerfile
FROM nvcr.io/nvidia/tensorflow:22.01-tf2-py3 AS tensorflow
RUN mkdir /nlp
WORKDIR /nlp
  ```
  
  ```docker-compose.yml:SudachiTra/docker-compose.yml
  version: '3'

services:
  sudachi:
    build: 
      context: .
      dockerfile: Dockerfile
      target: tensorflow
    tty: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - .:/nlp
    working_dir: /nlp
  ```
  
## ã‚³ãƒ³ãƒ†ãƒŠã®èµ·å‹•

 ã‚³ãƒ³ãƒ†ãƒŠã‚’èµ·å‹•ã—ã¾ã™ã€‚SudachiTraã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ã£ã¦ã‚³ãƒ³ãƒ†ãƒŠã‚’ç«‹ã¡ä¸Šã’ã¾ã™ã€‚
 
 ```bash : SudachiTra/
 # SudachiTra/ ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹
 docker-compose up -d
 ```
 
 ã€€ã‚³ãƒ³ãƒ†ãƒŠå†…ã«ã¯ã€

```bash : SudachiTra/
# SudachiTra/ ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹
# docker-compose up -dã®å®Ÿè¡Œå¾Œã«
docker-compose exec sudachi /bin/bash
```

ã§å…¥ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

:::message
ã‚‚ã—ã‚³ãƒ³ãƒ†ãƒŠã«å…¥ã‚Œãªã„å ´åˆã€```docker-composeã€€ps```ã‚³ãƒãƒ³ãƒ‰ã§ã‚³ãƒ³ãƒ†ãƒŠã®èµ·å‹•ã‚’ç¢ºèªã—ã¦ã¿ã¦ãã ã•ã„ã€‚
:::


# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
ã€€ã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã€[Training Sudachi BERT Modelsã€€set-up](https://github.com/WorksApplications/SudachiTra/tree/update/pretraining_document/pretraining/bert#set-up)ã«ã‚ã‚‹
 
 ``` bash:nlp/
 # ä¸Šè¨˜ã¨åŒã˜docker-compose.ymlã§ã‚ã‚Œã°ä»¥ä¸‹ã®cdã‚³ãƒãƒ³ãƒ‰ã§ç§»å‹•ã—ãŸå…ˆã§ä¸‹è¨˜ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã¡ã‚‡ã†ã©ã„ã„æ„Ÿã˜ã«ãªã‚Šã¾ã™ã€‚
 # cd SudachiTra/
 pip install -r requirements.txt
 pip install -r pretraining/bert/requirements.txt
 pip install -r pretraining/bert/models/official/requirements.txt

 # ã‚ã‚“ã©ãã•ã„ã®ã§ã€
 # pip install -r requirements.txt && pip install -r pretraining/bert/requirements.txt && pip install -r pretraining/bert/models/official/requirements.txt
 # ã§å®Ÿè¡Œã™ã‚‹ã¨ã„ã„ã¨æ€ã„ã¾ã™ã€‚
 ```

ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
ä¸Šè¨˜ã®docker-compose.ymlãªã‚‰/nlp/SudachiTra/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ä¸Šè¨˜ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚Œã°å¤§ä¸ˆå¤«ã§ã™ã€‚ãã‚Œãã‚Œã®ç’°å¢ƒã«åˆã‚ã›ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

:::message alert
ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•ã¨ã¨ã‚‚ã«ä¸Šè¨˜ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã‚ˆã†ã«Dockerfileã‚’æ›¸ã‘ã°ã„ã„ã®ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ã‚„ã‚Šæ–¹ã‚ã‹ã‚“ãªã‹ã£ãŸã®ã§ã€ã‚„ã‚Šæ–¹ã‚ã‹ã‚‹æ–¹ã„ã‚Œã°æ•™ãˆã¦ãã ã•ã„ã€‚
:::

**ä»¥ä¸‹å…¨ã¦ã‚³ãƒ³ãƒ†ãƒŠå†…ã§é€²ã‚ã¦ã„ãã¾ã™**


# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ãƒ³ãƒ­ãƒ¼ãƒ‰
[Quick Start ã®ã€€1.download-wiki40b](https://github.com/WorksApplications/SudachiTra/tree/update/pretraining_document/pretraining/bert#1-download-wiki40b)ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

ãŠæ¥ãšã‹ã—ãªãŒã‚‰wiki40bã¨ã„ã†å­˜åœ¨ã‚’çŸ¥ã‚‰ãšã«ä»Šã¾ã§éã”ã—ã¦ã„ã¾ã—ãŸã€‚wiki40bã¯æ—¥æœ¬èªã®å ´åˆã€2.19GB
ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãªã‚Šã¾ã™ã€‚

> 74ä¸‡5392ä»¶ã®è¨“ç·´ï¼ˆtrainï¼‰ãƒ‡ãƒ¼ã‚¿ç”¨
> 4ä¸‡1576ä»¶ã®ç²¾åº¦æ¤œè¨¼ï¼ˆvalidationï¼‰ãƒ‡ãƒ¼ã‚¿ç”¨
> 4ä¸‡1268ä»¶ã®ãƒ†ã‚¹ãƒˆï¼ˆtestï¼‰ãƒ‡ãƒ¼ã‚¿ç”¨
åˆè¨ˆ82ä¸‡8236ä»¶ï¼ˆ2.19GBï¼‰
[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ—¥æœ¬èªã®éƒ¨åˆ†å‚ç…§](https://www.tensorflow.org/datasets/catalog/wiki40b#wiki40bja)

ãã‚Œã§ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ãã¾ã™ã€‚

```bash:nlp/
#ã€€ä¸‹è¨˜ã®ã‚³ãƒãƒ³ãƒ‰ã‚’ã‚³ãƒ³ãƒ†ãƒŠå†…ã§å®Ÿè¡Œ
$ cd pretraining/bert/
# It may take several hours.
$ ./run_prepare_dataset.sh
```

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€Quick Startã«ã‚ˆã‚‹ã¨ã€
> BERTã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã«ã¯ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå˜ä½ã«åˆ†å‰²ã—ã¦æº–å‚™ã™ã‚‹å¿…è¦ãŒã‚ãƒªã¾ã™ã€‚ãã“ã§ã€‚run_prepare_dataset.shã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€wiki40bã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨å‡¦ç†ãŒè¡Œã‚ã‚Œã¾ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã™ã‚‹ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å†…å®¹ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚
> 
> ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€€: wiki40bãŒdatasets/corpusãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚
æ–‡ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ; ã‚³ãƒ¼ãƒ‘ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã¯åˆ¥ã€…ã®æ–‡ã«å‡¦ç†ã•ã‚Œã¾ã™ã€‚
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ : ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‘ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã€‚
> 
> å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯/datasets/corpus_splitted_by_paragraphã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚
ã‚³ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã¯åˆè¨ˆã§ç´„4.0GBã§ã™ã€‚

**å®Ÿè¡Œçµæœ**

```bash
745391 Elapsed Time: 13:04:37

real	786m2.272s
user	787m12.685s
sys	0m56.138s

--------------------------------
41575 Elapsed Time: 0:35:05

real	35m16.749s
user	35m18.906s
sys	0m5.971s

--------------------------------
41267 Elapsed Time: 0:35:00

real	35m11.358s
user	35m13.632s
sys	0m5.666s
```


# å‰å‡¦ç†

ã€€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚³ãƒ¼ãƒ‘ã‚¹ã®ä¸€éƒ¨ã«ã¯å­¦ç¿’ã«å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã«ã¯ä¸é©åˆ‡ï¼ˆçŸ­oré•·ã™ãã‚‹ï¼‰æ–‡ç« ãŒã‚ã‚Šã¾ã™ã€‚ãã®ã‚ˆã†ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’é™¤å¤–ã™ã‚‹ãŸã‚ã«ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

```bash:pretraining/bert/
python preprocess_dataset.py  \
-i ./datasets/corpus_splitted_by_paragraph/ja_wiki40b_train.paragraph.txt \
-o ./datasets/corpus_splitted_by_paragraph/ja_wiki40b_train.preprocessed.paragraph.txt \
--sentence_filter_names email url sequence_length \
--document_filter_names short_document script ng_words \
--sentence_normalizer_names citation whitespace \
--document_normalizer_names concat_short_sentence
```

**å®Ÿè¡Œçµæœ**

```bash
100% (1869817 of 1869817)  Elapsed Time: 0:04:46 Time:  0:04:46
```


# Vocablaryã®æ§‹ç¯‰
ã€€sudachiTraã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ã‚‹ã‚ˆã†ã«WordPieceã‚’ä½¿ç”¨ã—ã¦ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã€[Tokenizers](https://github.com/huggingface/tokenizers)ã§WordPieceã®å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã‚’ä½¿ã„ã¾ã—ãŸã€‚
 

### WordPiece

```bash:pretraining/bert/
python train_wordpiece_tokenizer.py \
--input_file datasets/corpus_splitted_by_paragraph/ja_wiki40b_train.preprocessed.paragraph.txt \
--do_nfkc \
--vocab_size 32000 \
--limit_alphabet 5000 \
--dict_type core \
--split_mode C \
--word_form_type normalized \
--output_dir _tokenizers/ja_wiki40b/wordpiece/train_CoreDic_normalized_unit-C \
--config_name config.json \
--vocab_prefix wordpiece
```

**å®Ÿè¡Œçµæœ**

```
[03:30:17] Pre-processing files (1528 Mo)               100%
[00:00:01] Tokenize words  1215394  /  1215394
[00:00:01] Count pairs     1215394  /  1215394
[00:00:06] Compute merges  22320    /    22320
```

### Character

ã€€sudachiTraã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ã‚‹ã‚ˆã†ã«wordPieceã«ã‚ã‚‹ã‚ˆã†ãªãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸCharacterã«ã‚ˆã£ã¦ä½œæˆã•ã‚ŒãŸèªå½™ã‹ã‚‰æ–‡å­—ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹ã“ã¨ã§ã€èªå½™ã‚’å–å¾—ã§ãã¾ã™ã€‚

```bash:pretraining/bert/
OUTPUT_DIR="tokenizers/ja_wiki40b/character/train_CoreDic_normalized_unit-C"
mkdir -p $OUTPUT_DIR
head -n 5005 _tokenizers/ja_wiki40b/wordpiece/train_CoreDic_normalized_unit-C/wordpiece-vocab.txt > $OUTPUT_DIR/vocab.txt
```

### POS Substitution(part-of-speech substitution : å“è©ç½®æ›)

ã€€POS Substitutionã¯ã€å“è©ã‚¿ã‚°ã‚’ä½¿ç”¨ã—ã¦èªå½™ã®ã‚µã‚¤ã‚ºã‚’ç¸®å°ã™ã‚‹æ–¹æ³•ã§ã™ã€‚
ã€€POS Substitutionã€ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ä»£ã‚ã‚Šã«ã€ä½é »åº¦ã®å˜èªãŒå“è©ã‚¿ã‚°ã«ç½®ãæ›ãˆã‚‰ã‚Œã¾ã™ã€‚
ã€€æœ€å¾Œã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‘ã‚¹ã«è¡¨ç¤ºã•ã‚Œãªã„å“è©ã‚¿ã‚°ã®ã¿ãŒä¸æ˜ãªå˜èªã¨ã—ã¦æ‰±ã‚ã‚Œã¾ã™ã€‚

```bash:pretraining/bert/
python train_pos_substitution_tokenizer.py \
--input_file datasets/corpus_splitted_by_paragraph/ja_wiki40b_train.preprocessed.paragraph.txt \
--token_size 32000 \
--limit_character 5000 \
--dict_type core \
--split_mode C \
--word_form_type normalized \
--output_file _tokenizers/ja_wiki40b/pos_substitution/train_CoreDic_normalized_unit-C/vocab.txt 
```

# äº‹å‰å­¦ç¿’ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ

ã€€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã™ã‚‹ã“ã¨ãŒsudaxhiTraã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§æ¨å¥¨ã•ã‚Œã¦ã„ã‚‹ã®ã§ã—ãŸãŒã£ã¦åˆ†å‰²ã—ã¾ã™ã€‚
ã€€ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã•ã‚ŒãŸsudachiTraã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯è¿½åŠ ã§ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«é–¢ã™ã‚‹èª¬æ˜ã¨å®Ÿè¡Œã«å¿…è¦ãªãƒªã‚½ãƒ¼ã‚¹ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚
ã€€ã“ã®åˆ†å‰²ç”¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯[TensorFlow ModelGarden](https://github.com/tensorflow/models)ã«åŸºã¥ã„ã¦ã„ã‚‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚

> [å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://github.com/WorksApplications/SudachiTra/tree/main/pretraining/bert#4creating-data-for-pretraining)
> It consumes about 10 GB or more of memory to create the data for pre-training from this one file.

ã¨å¤§é‡ã«ãƒ¡ãƒ¢ãƒªãŒæ¶ˆè²»ã•ã‚Œã‚‹ã“ã¨ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ãŒã€**æœ¬å½“ã«ä¿¡ã˜ã‚‰ã‚Œãªã„ãã‚‰ã„æ¶ˆè²»ã—ã¾ã™**

ä¾‹ã«ã—ãŸãŒã£ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®æ–‡æ•°ï¼ˆ--line_per_fileï¼‰ã¯700,000ã«è¨­å®šã—ã¦ã€å®Ÿè¡Œã™ã‚‹ã¨æœ€å¤§ã§è¦‹ãŸé™ã‚Š89GBãã‚‰ã„ä½¿ã£ã¦ã¾ã—ãŸã€‚
ãŸã ã€ç’°å¢ƒãŒ68GBã ã£ãŸã®ã§ã€MAX_PROCSã‚’6ã«è¨­å®šã—ã¦ã„ã¾ã™ã€‚

```bash:pretraining/bert/
ï¼ƒ wiki40bã‚’è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²
python split_dataset.py \
--input_filedatasets / corpus_splitted_by_paragraph / ja_wiki40b_train.preprocessed.paragraph.txt \
--line_per_file 700000

TRAIN_FILE_NUM = ` finddatasets / corpus_splitted_by_paragraph -type f | grep -E " ja_wiki40b_train.preprocessed.paragraph [0-9] +ã€‚txt "  | wc -l `
```

```bash : nlp/pretrainig/bert/
cd ../../ 
MAX_PROCS = 8 
mkdir datasets_for_pretraining
export PYTHONPATH="$PYTHONPATH:/nlp/pretraining/bert/models"

seq -f %20g 1 ${TRAIN_FILE_NUM} | xargs -L 1 -I {} -P ${MAX_PROCS} python3 models/official/nlp/data/create_pretraining_data.py \
--input_file datasets/corpus_splitted_by_paragraph/ja_wiki40b_train.preprocessed.paragraph{}.txt \
--output_file datasets_for_pretraining/pretraining_train_{}.tf_record \
--do_nfkc \
--vocab_file _tokenizers/ja_wiki40b/wordpiece/train_CoreDic_normalized_unit-C/wordpiece-vocab.txt \
--tokenizer_type wordpiece \
--word_form_type normalized \
--split_mode C \
--sudachi_dic_type core \
--do_whole_word_mask \
--max_seq_length 512 \
--max_predictions_per_seq 80 \
--dupe_factor 10
```

:::message alert
```seq -f %20g 1 ${TRAIN_FILE_NUM} ```ã®éƒ¨åˆ†ã§ã™ãŒã€å…¬å¼ã§ã¯```seq -f 1 ${TRAIN_FILE_NUM} ```ã¨ãªã£ã¦ã„ã¾ã™ãŒã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æŒ‡å®šãªã„ã¨ã ã‚ãªã‚“ã˜ã‚ƒãªã„ã‹ã¨å€‹äººçš„ã«æ€ã£ã¦ã„ã¦ã€```%20g```ã‚’åŠ ãˆã¾ã—ãŸã€‚
ã“ã‚Œã«é–¢ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£Slackã§ç¢ºèªä¸­ã§ã™ã€‚
:::

:::message alert
ã€€Pythonã®```export PYTHONPATH="$PYTHONPATH:/nlp/pretraining/bert/models"```ã®éƒ¨åˆ†ã§ã™ãŒã€æœ¬æ¥ã¯ã€docker-compose.ymlã«è¨˜è¼‰ã—ãŸã»ã†ãŒã„ã„ã®ã¨æ€ã†ã®ã§ã™ãŒã€ã“ã‚Œã‚‚ã‚ã‚“ã©ãã•ã‹ã£ãŸã®ã§ã‹ã£é£›ã°ã—ã¾ã—ãŸã€‚
:::

**å®Ÿè¡Œçµæœ**
ã€€ã¨ã‚“ã§ã‚‚ãªãé•·ã„ã®ã§çœç•¥ã—ã¾ã™ãŒã€å‰²ã¨æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚

# å­¦ç¿’
ã€€ã‚„ã£ã¨å­¦ç¿’ã§ã™ï¼

ã€€GPU4æšä½¿ç”¨ã™ã‚‹å ´åˆã§ã™ã€‚
ã€€
ã€€sudachiTraã®æ–°ã—ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«é–¢ã—ã¦ã¯ã€LAMBãŒåˆ©ç”¨ã§ãã‚‹ã®ã§ã€nvidiaã®Dockerã‚³ãƒ³ãƒ†ãƒŠãŒæ¨å¥¨ã•ã‚Œã¦ã„ã¾ã™ãŒã€tensorflowã§äº‹é …ã—ã¦ã—ã¾ã£ãŸã®ã§ã€å…ˆã«ãã¡ã‚‰ã‚’è¨˜è¼‰ã—ã¾ã™ã€‚

```bash : /nlp/pretraining/bert/
pwd
# /nlp/pretraining/bert/
cd models/
# export BERT_PREP_WORKING_DIR="/workspace/bert/data"

WORK_DIR="../../../pretraining/bert"; python official/nlp/bert/run_pretraining.py \
--input_files="$WORK_DIR/datasets_for_pretraining/pretraining_*_*.tf_record" --model_dir="$WORK_DIR/bert_small/" \
--bert_config_file="$WORK_DIR/bert_small/bert_small_config.json" \
--max_seq_length=512 \
--max_predictions_per_seq=80 \
--train_batch_size=1 \
--learning_rate=1e-4 \
--num_train_epochs=100 \
--num_steps_per_epoch=10000 \
--optimizer_type=adamw \
--warmup_steps=10000 \
--num_gpus=4
```

:::message
```--num_gpus=2```ã¯ä½¿ç”¨ã™ã‚‹GPUã®æšæ•°ã‚’æŒ‡å®šã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ã€‚
ã™ã¹ã¦ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¦‹ã‚‹ã«ã¯ã€```--helpfull```ã§è¦‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
:::