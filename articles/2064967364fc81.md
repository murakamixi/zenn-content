---
title: "SudachiをつかったBERTの事前学習モデルの構築をしてみようとおもったら（いまやってる途中）"
emoji: "🙄"
type: "tech"
topics: ["bert", "sudachi", "transformer", "事前学習", "sudachitra"]
published: true
---

これは個人的な作業記録的なものと、もし今後同じような志を持った人の参考となるようにするために可能な限り作業の詳細を残しています。

# そもそもSudachiとは
　そもそもSudachiってなにてきなひとはこんなマニアックなタイトルの記事のところに来ないでしょうがもしも気になる方は僕の友人が、[形態素解析器Sudachiをpythonファイルで使ってみた](https://zenn.dev/2timesbottle/articles/4c0f3a4ce26797)的な記事を書いているので的に参照してみるのがいいとお思います。
 
# なんで事前学習モデルなんか作ろうと思ったのか？
 　山形大学の理学部に在籍していて、ちょうど卒業研究終わりなどで、計算リソースがほぼ独占状態でつかえること
  計算リソースも一応国立大というだけあってBERTの事前学習に耐えられるだけありそう
  研究室配属前でわりと好きなことができて、時間がある期間ということ
  
  という感じにべつに絶対やり遂げようとおもってるわけでもないのですが、なにより「**ないからなんとなくつくってみようかな**」てきな興味本位ではじめました。
  
# 手順
  
  　手順はすべて[Training Sudachi BERT Models](https://github.com/WorksApplications/SudachiTra/tree/update/pretraining_document/pretraining/bert)に書いてあります。
   いろいろと古いものも玉石混合だったようですが、Slack上で聞くと丁寧に対応していただける開発者の方がいて古いものやエラーは一掃されたように思えます。
   まだやってる途中なのでわかりません！
   
   以下は上記のドキュメントプラス個人的な環境のものなど入れてます。

[環境構築](環境構築)
[必要なパッケージのインストール](必要なパッケージのインストール)
   
# 環境構築

環境構築は、
1.SudachiTraをGithubからClone
2.Dockerコンテナの準備
で行います。

## SudachiTraをGithubからClone

　ディレクトリないと始まらないので、[Training Sudachi BERT Models　set-up](https://github.com/WorksApplications/SudachiTra/tree/update/pretraining_document/pretraining/bert#set-up)のはじめにある

```bash : ターミナル
git clone --recursive https://github.com/WorksApplications/SudachiTra/
pip install -U sudachitra
```

のコマンドを適当なターミナルで実行します。

::: message
その後、このディレクトリに環境構築用のDockerコンテナの準備をします。
:::

## Dockerの準備
 　環境は、[NGCプラットフォームのTensorflowコンテナ](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow)を利用しました。
 　Dockerfileとdocker-compose.ymlは以下のような感じです。
  
  ```Dockerfile:SudachiTra/Dockerfile
FROM nvcr.io/nvidia/tensorflow:22.01-tf2-py3 AS tensorflow
RUN mkdir /nlp
WORKDIR /nlp
  ```
  
  ```docker-compose.yml:SudachiTra/docker-compose.yml
  version: '3'

services:
  sudachi:
    build: 
      context: .
      dockerfile: Dockerfile
      target: tensorflow
    tty: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - .:/nlp
    working_dir: /nlp
  ```
  
## コンテナの起動

 コンテナを起動します。SudachiTraのディレクトリで以下のコマンドを使ってコンテナを立ち上げます。
 
 ```bash : bash
 # SudachiTra/ のディレクトリで以下のコマンドを実行する
 docker-compose up -d
 ```
 
 　コンテナ内には、

```bash : bash
# SudachiTra/ のディレクトリで以下のコマンドを実行する
# docker-compose up -dの実行後に
docker-compose exec sudachi /bin/bash
```

で入ることができます。

:::message
もしコンテナに入れない場合、```docker-compose　ps```コマンドでコンテナの起動を確認してみてください。
:::


# 必要なパッケージのインストール
　コンテナ内で、[Training Sudachi BERT Models　set-up](https://github.com/WorksApplications/SudachiTra/tree/update/pretraining_document/pretraining/bert#set-up)にある
 
 ``` bash:bash
 # 上記と同じdocker-compose.ymlであれば以下のcdコマンドで移動した先で下記のコマンドを実行するとちょうどいい感じになります。
 # cd SudachiTra/
 pip install -r requirements.txt
 pip install -r pretraining/bert/requirements.txt
 pip install -r pretraining/bert/models/official/requirements.txt
 ```

のコマンドを実行します。
上記のdocker-compose.ymlなら/nlp/SudachiTra/ディレクトリで上記のコマンドを実行すればいいかと思います。

:::message alert
コンテナ起動とともに上記のコマンドを実行するようにDockerfileを書けばいいのかもしれませんが、やり方わかんなかったので、やり方わかる方いれば教えてください。
:::

**以下全てコンテナ内で進めていきます**


# データセットのダンロード
[Quick Start の　1.download-wiki40b](https://github.com/WorksApplications/SudachiTra/tree/update/pretraining_document/pretraining/bert#1-download-wiki40b)をダウンロードします。

お恥ずかしながらwiki40bという存在を知らずに今まで過ごしていました。wiki40bは日本語の場合、2.19GB
のデータセットからなります。

> 74万5392件の訓練（train）データ用
> 4万1576件の精度検証（validation）データ用
> 4万1268件のテスト（test）データ用
合計82万8236件（2.19GB）
[公式ドキュメントの日本語の部分参照](https://www.tensorflow.org/datasets/catalog/wiki40b#wiki40bja)

それではダウンロードしていきます。

```
#　下記のコマンドをコンテナ内で実行
$ cd pretraining/bert/
# It may take several hours.
$ ./run_prepare_dataset.sh
```

このファイルは、Quick Startによると、
> BERTの事前トレーニングのためにはデータをドキュメント単位に分割して準備する必要があリます。そこで。run_prepare_dataset.shスクリプトを実行すると、wiki40bのダウンロードと処理が行われます。
データセットを準備するためのスクリプトの内容は次のとおりです。
> 
> データのダウンロード　: wiki40bがdatasets/corpusディレクトリにダウンロードされます。
文のセグメンテーション ; コーパステキストは別々の文に処理されます。
ドキュメントのセグメンテーション : ドキュメントに分割されたコーパステキスト。
> 
> 処理されたデータは/datasets/corpus_splitted_by_paragraphに保存されます。
コーパスファイルは合計で約4.0GBです。


 今はこのコードを実行中です。

  